{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8acb9d04-9d73-4e57-9b80-5d9a1bbc8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dff9688-f139-4743-8ca0-08ccbf67cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "635dcbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHIDPN</th>\n",
       "      <th>S1HHIDPN</th>\n",
       "      <th>R1MSTAT</th>\n",
       "      <th>R1MPART</th>\n",
       "      <th>S1BMONTH</th>\n",
       "      <th>S1BYEAR</th>\n",
       "      <th>S1BDATE</th>\n",
       "      <th>S1BFLAG</th>\n",
       "      <th>S1COHBYR</th>\n",
       "      <th>S1HRSAMP</th>\n",
       "      <th>...</th>\n",
       "      <th>R8LBSATWLF</th>\n",
       "      <th>R9LBSATWLF</th>\n",
       "      <th>R10LBSATWLF</th>\n",
       "      <th>R11LBSATWLF</th>\n",
       "      <th>R12LBSATWLF</th>\n",
       "      <th>R13LBSATWLF</th>\n",
       "      <th>R14LBSATWLF</th>\n",
       "      <th>R15LBSATWLF</th>\n",
       "      <th>R16LBSATWLF</th>\n",
       "      <th>FILEVER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3010</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>-7778.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3020</td>\n",
       "      <td>3010.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1936.0</td>\n",
       "      <td>-8752.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19880 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HHIDPN  S1HHIDPN  R1MSTAT  R1MPART  S1BMONTH  S1BYEAR  S1BDATE  S1BFLAG  \\\n",
       "0      1010       0.0      5.0      0.0       NaN      NaN      NaN      NaN   \n",
       "1      2010       0.0      7.0      0.0       NaN      NaN      NaN      NaN   \n",
       "2      3010    3020.0      1.0      0.0       9.0   1938.0  -7778.0      0.0   \n",
       "3      3020    3010.0      1.0      0.0       1.0   1936.0  -8752.0      0.0   \n",
       "4  10001010       0.0      8.0      0.0       NaN      NaN      NaN      NaN   \n",
       "\n",
       "   S1COHBYR  S1HRSAMP  ...  R8LBSATWLF  R9LBSATWLF  R10LBSATWLF  R11LBSATWLF  \\\n",
       "0       NaN       NaN  ...         NaN         NaN          NaN          NaN   \n",
       "1       NaN       NaN  ...         NaN         NaN          NaN          NaN   \n",
       "2       3.0       1.0  ...         5.4         NaN          6.4          NaN   \n",
       "3       3.0       1.0  ...         4.6         NaN          4.2          NaN   \n",
       "4       NaN       NaN  ...         NaN         2.8          NaN          4.8   \n",
       "\n",
       "   R12LBSATWLF  R13LBSATWLF  R14LBSATWLF  R15LBSATWLF  R16LBSATWLF  FILEVER  \n",
       "0          NaN          NaN          NaN          NaN          NaN        X  \n",
       "1          NaN          NaN          NaN          NaN          NaN        X  \n",
       "2          NaN          NaN          NaN          NaN          NaN        X  \n",
       "3          2.4          NaN          NaN          NaN          NaN        X  \n",
       "4          NaN          NaN          NaN          NaN          NaN        X  \n",
       "\n",
       "[5 rows x 19880 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_stata('data/extracted/randhrs1992_2022v1.dta', convert_categoricals=False)\n",
    "# df.to_parquet('data/extracted/randhrs1992_2022v1.parquet')\n",
    "# df.columns = df.columns.str.upper()\n",
    "df = pd.read_parquet('data/extracted/randhrs1992_2022v1.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e22373-5613-4325-94f2-02b4150abe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAVE_YEARS = {\n",
    "    1: 1992,  # HRS cohort entry\n",
    "    2: 1993,  # AHEAD cohort entry (off-year)\n",
    "    3: 1994,  # Off-year\n",
    "    4: 1995,  # Off-year\n",
    "    5: 1996,  # Biennial pattern begins\n",
    "    6: 1998,  # CODA/WB cohorts enter\n",
    "    7: 2000,\n",
    "    8: 2002,\n",
    "    9: 2004,  # Early Baby Boomer cohort enters\n",
    "    10: 2006,\n",
    "    11: 2008,\n",
    "    12: 2010, # Mid Baby Boomer cohort enters\n",
    "    13: 2012,\n",
    "    14: 2014,\n",
    "    15: 2016, # Late Baby Boomer cohort enters\n",
    "    16: 2018,\n",
    "    17: 2020, # COVID wave\n",
    "    18: 2022  # Early Generation X cohort enters\n",
    "}\n",
    "\n",
    "def get_wave_year(wave: int) -> int | float:\n",
    "    return WAVE_YEARS.get(wave, np.nan)\n",
    "\n",
    "def get_variable_name(wave: int, var_base: str) -> str:\n",
    "    return f\"R{wave}{var_base}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d8239f-2054-4497-a6a2-b7157397e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wave_features(df: pd.DataFrame, wave: int, lags: int = 2):\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # Base identifiers\n",
    "    data['person_id'] = df['HHIDPN']\n",
    "    data['wave'] = wave\n",
    "    data['wave_year'] = WAVE_YEARS[wave]\n",
    "\n",
    "    # Demographics\n",
    "    birth_year = pd.to_numeric(df['RABYEAR'], errors='coerce')\n",
    "    age = WAVE_YEARS[wave] - birth_year # pyright: ignore[reportOperatorIssue]\n",
    "\n",
    "    data['birth_year'] = birth_year\n",
    "    data['age'] = age\n",
    "    data['age_squared'] = age ** 2\n",
    "\n",
    "    data['female'] = (pd.to_numeric(df['RAGENDER'], errors='coerce') == 2)\n",
    "    data['white'] = (pd.to_numeric(df['RARACEM'], errors='coerce') == 1)\n",
    "    data['black'] = (pd.to_numeric(df['RARACEM'], errors='coerce') == 2)\n",
    "    data['hispanic'] = (pd.to_numeric(df['RAHISPAN'], errors='coerce') == 1)\n",
    "\n",
    "    data['education_years'] = pd.to_numeric(df['RAEDUC'], errors='coerce')\n",
    "    data['college_plus'] = (pd.to_numeric(df['RAEDEGRM'], errors='coerce') >= 4)  # pyright: ignore[reportOperatorIssue]\n",
    "\n",
    "    variables = {\n",
    "        'self_rated_health': 'SHLT',\n",
    "        'bmi': 'BMI',\n",
    "        'weight': 'WEIGHT',\n",
    "        'height': 'HEIGHT',\n",
    "        'mobility_limitations': 'MOBILA',\n",
    "        'large_muscle_limitations': 'LGMUSA',\n",
    "        'adl_limitations': 'ADL5A',\n",
    "        'iadl_limitations': 'IADL5A',\n",
    "        'fine_motor_limitations': 'FINEA',\n",
    "        'cognition_score': 'COG27',\n",
    "        'memory_recall': 'TR20',\n",
    "        'immediate_recall': 'IMRC',\n",
    "        'delayed_recall': 'DLRC',\n",
    "        'serial7': 'SER7',\n",
    "        'depression_score': 'CESD',\n",
    "        'felt_depressed': 'DEPRES',\n",
    "        'everything_effort': 'EFFORT',\n",
    "        'restless_sleep': 'SLEEPR',\n",
    "        'felt_lonely': 'FLONE',\n",
    "        'ever_smoked': 'SMOKEV',\n",
    "        'current_smoker': 'SMOKEN',\n",
    "        'drinks_per_day': 'DRINKD',\n",
    "        'drink_days_per_week': 'DRINKN',\n",
    "        'vigorous_activity': 'VIGACT',\n",
    "        'marital_status': 'MSTAT'\n",
    "    }\n",
    "\n",
    "    # Loop from 0 to lags\n",
    "    for lag in range(lags + 1):\n",
    "        w = wave - lag\n",
    "        if w < 1:\n",
    "            continue\n",
    "\n",
    "        suffix = f\"_lag{lag}\" if lag > 0 else \"\"\n",
    "\n",
    "        for name, code in variables.items():\n",
    "            col = f'R{w}{code}'\n",
    "            data[f'{name}{suffix}'] = df.get(col)\n",
    "\n",
    "    features = pd.DataFrame(data)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c52e38-95bd-43ec-a811-d354a965e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_features(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Add temporal features to existing features DataFrame.\n",
    "    Avoids DataFrame fragmentation by concatenating once.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_features = {}\n",
    "\n",
    "    # ========================================\n",
    "    # BMI TRAJECTORIES\n",
    "    # ========================================\n",
    "    new_features['bmi_velocity_2yr'] = (df['bmi'] - df['bmi_lag1']) / 2\n",
    "    new_features['bmi_velocity_4yr'] = (df['bmi'] - df['bmi_lag2']) / 4\n",
    "    new_features['bmi_acceleration'] = ((df['bmi'] - df['bmi_lag1']) / 2) - ((df['bmi_lag1'] - df['bmi_lag2']) / 2)\n",
    "    new_features['weight_change_kg'] = df['weight'] - df['weight_lag1']\n",
    "    new_features['weight_change_pct'] = (new_features['weight_change_kg'] / df['weight_lag1']) * 100\n",
    "    new_features['obese'] = (df['bmi'] >= 30).astype(int)\n",
    "    new_features['overweight'] = ((df['bmi'] >= 25) & (df['bmi'] < 30)).astype(int)\n",
    "    new_features['normal_weight'] = ((df['bmi'] >= 18.5) & (df['bmi'] < 25)).astype(int)\n",
    "    new_features['rapid_weight_gain'] = (new_features['bmi_velocity_2yr'] > 1).astype(int)\n",
    "    new_features['rapid_weight_loss'] = (new_features['bmi_velocity_2yr'] < -1).astype(int)\n",
    "\n",
    "    # ========================================\n",
    "    # HEALTH PERCEPTION\n",
    "    # ========================================\n",
    "    new_features['health_decline_2yr'] = df['self_rated_health'] - df['self_rated_health_lag1']\n",
    "    new_features['health_decline_4yr'] = df['self_rated_health'] - df['self_rated_health_lag2']\n",
    "    new_features['health_worsening'] = (new_features['health_decline_2yr'] > 0).astype(int)\n",
    "    new_features['health_crash'] = (new_features['health_decline_2yr'] >= 2).astype(int)\n",
    "\n",
    "    # ========================================\n",
    "    # FUNCTIONAL DECLINE\n",
    "    # ========================================\n",
    "    new_features['mobility_decline_2yr'] = df['mobility_limitations'] - df['mobility_limitations_lag1']\n",
    "    new_features['mobility_worsening'] = (new_features['mobility_decline_2yr'] > 0).astype(int)\n",
    "    new_features['new_mobility_problem'] = ((df['mobility_limitations'] > 0) & (df['mobility_limitations_lag1'] == 0)).astype(int)\n",
    "    new_features['any_mobility_limitation'] = (df['mobility_limitations'] > 0).astype(int)\n",
    "\n",
    "    new_features['adl_decline_2yr'] = df['adl_limitations'] - df['adl_limitations_lag1']\n",
    "    new_features['adl_worsening'] = (new_features['adl_decline_2yr'] > 0).astype(int)\n",
    "    new_features['new_adl_problem'] = ((df['adl_limitations'] > 0) & (df['adl_limitations_lag1'] == 0)).astype(int)\n",
    "    new_features['any_adl_limitation'] = (df['adl_limitations'] > 0).astype(int)\n",
    "\n",
    "    new_features['iadl_decline_2yr'] = df['iadl_limitations'] - df['iadl_limitations_lag1']\n",
    "    new_features['any_iadl_limitation'] = (df['iadl_limitations'] > 0).astype(int)\n",
    "\n",
    "    # ========================================\n",
    "    # COGNITIVE DECLINE\n",
    "    # ========================================\n",
    "    new_features['cognition_decline_2yr'] = df['cognition_score_lag1'] - df['cognition_score']\n",
    "    new_features['cognition_decline_4yr'] = df['cognition_score_lag2'] - df['cognition_score']\n",
    "    new_features['cognition_worsening'] = (new_features['cognition_decline_2yr'] > 0).astype(int)\n",
    "    new_features['sharp_cognitive_drop'] = (new_features['cognition_decline_2yr'] > 3).astype(int)\n",
    "    new_features['low_cognition'] = (df['cognition_score'] < 12).astype(int)\n",
    "    new_features['memory_decline_2yr'] = df['memory_recall_lag1'] - df['memory_recall']\n",
    "    new_features['memory_worsening'] = (new_features['memory_decline_2yr'] > 0).astype(int)\n",
    "\n",
    "    # ========================================\n",
    "    # MENTAL HEALTH\n",
    "    # ========================================\n",
    "    new_features['depression_increase_2yr'] = df['depression_score'] - df['depression_score_lag1']\n",
    "    new_features['depression_worsening'] = (new_features['depression_increase_2yr'] > 0).astype(int)\n",
    "    new_features['elevated_depression'] = (df['depression_score'] >= 3).astype(int)\n",
    "    new_features['high_depression'] = (df['depression_score'] >= 4).astype(int)\n",
    "    new_features['chronic_depression'] = ((df['depression_score'] >= 3) & (df['depression_score_lag1'] >= 3)).astype(int)\n",
    "\n",
    "    # ========================================\n",
    "    # HEALTH BEHAVIORS\n",
    "    # ========================================\n",
    "    new_features['former_smoker'] = ((df['ever_smoked'] == 1) & (df['current_smoker'] == 0)).astype(int)\n",
    "    new_features['quit_smoking'] = ((df['current_smoker'] == 0) & (df['current_smoker_lag1'] == 1)).astype(int)\n",
    "    new_features['sedentary'] = (df['vigorous_activity'] == 0).astype(int)\n",
    "    new_features['physically_active'] = (df['vigorous_activity'] == 1).astype(int)\n",
    "    new_features['stopped_activity'] = ((new_features['physically_active'] == 0) & (df['vigorous_activity_lag1'] == 1)).astype(int)\n",
    "    new_features['drinks_per_week'] = df['drinks_per_day'] * df['drink_days_per_week']\n",
    "    new_features['heavy_drinking'] = (new_features['drinks_per_week'] > 14).astype(int)\n",
    "\n",
    "    # ========================================\n",
    "    # INTERACTIONS\n",
    "    # ========================================\n",
    "    new_features['age_x_bmi'] = df['age'] * df['bmi']\n",
    "    new_features['sedentary_and_obese'] = new_features['sedentary'] * new_features['obese']\n",
    "    new_features['depression_x_mobility'] = df['depression_score'] * df['mobility_limitations']\n",
    "    new_features['cognitive_decline_x_age'] = new_features['cognition_decline_2yr'] * df['age']\n",
    "\n",
    "    # ========================================\n",
    "    # COMPOSITE SCORES\n",
    "    # ========================================\n",
    "    new_features['metabolic_risk_score'] = new_features['obese'] * 2 + new_features['sedentary'] + (df['age'] >= 65).astype(int)\n",
    "    new_features['frailty_indicators'] = (df['mobility_limitations'] > 0).astype(int) + (df['depression_score'] >= 3).astype(int) + (new_features['weight_change_pct'] < -5).astype(int)\n",
    "\n",
    "    # Single concat to prevent fragmentation\n",
    "    df = pd.concat([df, pd.DataFrame(new_features, index=df.index)], axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0676c07-764f-4c75-afb4-144d2f42cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_map = {\n",
    "    'diabetes': 'DIABE',\n",
    "    'cvd': 'HEARTE', # cardiovascular disease\n",
    "    'stroke': 'STROK',\n",
    "    'lung': 'LUNGE',\n",
    "    'cancer': 'CANCRE',\n",
    "    'hibp': 'HIBPE', # high blood preasure \n",
    "    'arthritis': 'ARTHRE'\n",
    "}\n",
    "\n",
    "def create_targets(df: pd.DataFrame, feature_wave: int, outcome_wave: int):\n",
    "\n",
    "    new_targets = {}\n",
    "    new_targets['person_id'] = df['HHIDPN']\n",
    "\n",
    "    for target_name, code in disease_map.items():\n",
    "        baseline_col = f'R{feature_wave}{code}'\n",
    "        outcome_col = f'R{outcome_wave}{code}'\n",
    "\n",
    "        if baseline_col in df.columns and outcome_col in df.columns:\n",
    "            no_disease_baseline = df[baseline_col] == 0\n",
    "            develops_disease = df[outcome_col] == 1\n",
    "\n",
    "            target_values = (no_disease_baseline & develops_disease).astype(float)\n",
    "            target_values[df[outcome_col].isna()] = np.nan\n",
    "\n",
    "            new_targets[f'target_{target_name}'] = target_values\n",
    "            new_targets[f'eligible_{target_name}'] = no_disease_baseline.astype(int)\n",
    "\n",
    "    # Single DataFrame creation → no fragmentation\n",
    "    targets = pd.DataFrame(new_targets, index=df.index)\n",
    "\n",
    "    return targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4411d5e3-c4e0-4052-ad01-9be42b3d9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    df: pd.DataFrame, \n",
    "    feature_waves: List[int], \n",
    "    prediction_horizon: int = 2, \n",
    "    target: str = 'diabetes'\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a modeling dataset for a specific disease target.\n",
    "    \"\"\"\n",
    "    # Ensure standard naming convention\n",
    "    if 'hhidpn' in df.columns:\n",
    "        df.columns = df.columns.str.upper()\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for wave in feature_waves:\n",
    "        outcome_wave = wave + prediction_horizon\n",
    "        if outcome_wave > 16:\n",
    "            continue\n",
    "        \n",
    "        # 1. Extract wave features and add temporal math (lags, velocity)\n",
    "        feats = extract_wave_features(df, wave)\n",
    "        feats = add_temporal_features(feats)\n",
    "        \n",
    "        # 2. Create target and eligibility flags for this specific horizon\n",
    "        targs = create_targets(df, wave, outcome_wave)\n",
    "        \n",
    "        # 3. Join features and targets on unique ID\n",
    "        combined = feats.merge(targs, on='person_id', how='inner')\n",
    "        all_data.append(combined)\n",
    "    \n",
    "    # Combine all selected waves into one master dataframe\n",
    "    dataset = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    \n",
    "    # --- FILTERING FOR THE SPECIFIC TARGET ---\n",
    "    eligible_col = f'eligible_{target}'\n",
    "    target_col = f'target_{target}'\n",
    "    \n",
    "    # Only keep people who did NOT have the disease at the feature wave\n",
    "    dataset = dataset[dataset[eligible_col] == 1].copy()\n",
    "    \n",
    "    # Remove rows where the outcome is unknown (NaN)\n",
    "    dataset = dataset[pd.notna(dataset[target_col])].copy()\n",
    "\n",
    "    # Drop rows missing critical predictors to ensure model quality\n",
    "    dataset = dataset.dropna(subset=('age', 'bmi'))  # pyright: ignore[reportCallIssue]\n",
    "    \n",
    "    # --- FEATURE SELECTION ---\n",
    "    # We must exclude ID, metadata, and ALL target/eligible flags to prevent leakage\n",
    "    exclude_prefixes = ('target_', 'eligible_')\n",
    "    metadata_cols = ['person_id', 'wave', 'wave_year']\n",
    "    \n",
    "    feature_cols = [c for c in dataset.columns \n",
    "                    if not c.startswith(exclude_prefixes) \n",
    "                    and c not in metadata_cols]\n",
    "    \n",
    "    X = dataset[feature_cols].copy()\n",
    "    y = dataset[target_col].copy()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c31edc3-d8c6-4a06-9a2d-aac1ecfa4bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataset for diabetes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124450/1075262581.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataset = pd.concat(all_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes: shape = (98033, 134), positives = 6357.0, saved to data/processed/randhrs_diabetes_4yr.csv & data/processed/randhrs_diabetes_4yr.parquet\n",
      "\n",
      "Creating dataset for cvd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124450/1075262581.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataset = pd.concat(all_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvd: shape = (96166, 134), positives = 7888.0, saved to data/processed/randhrs_cvd_4yr.csv & data/processed/randhrs_cvd_4yr.parquet\n",
      "\n",
      "Creating dataset for stroke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124450/1075262581.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataset = pd.concat(all_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stroke: shape = (112441, 134), positives = 3469.0, saved to data/processed/randhrs_stroke_4yr.csv & data/processed/randhrs_stroke_4yr.parquet\n",
      "\n",
      "Creating dataset for lung...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124450/1075262581.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataset = pd.concat(all_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lung: shape = (111273, 134), positives = 3762.0, saved to data/processed/randhrs_lung_4yr.csv & data/processed/randhrs_lung_4yr.parquet\n",
      "\n",
      "Creating dataset for cancer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124450/1075262581.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataset = pd.concat(all_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer: shape = (105724, 134), positives = 4969.0, saved to data/processed/randhrs_cancer_4yr.csv & data/processed/randhrs_cancer_4yr.parquet\n",
      "\n",
      "Creating dataset for hibp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124450/1075262581.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataset = pd.concat(all_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hibp: shape = (57089, 134), positives = 10084.0, saved to data/processed/randhrs_hibp_4yr.csv & data/processed/randhrs_hibp_4yr.parquet\n",
      "\n",
      "Creating dataset for arthritis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124450/1075262581.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataset = pd.concat(all_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arthritis: shape = (55325, 134), positives = 9486.0, saved to data/processed/randhrs_arthritis_4yr.csv & data/processed/randhrs_arthritis_4yr.parquet\n"
     ]
    }
   ],
   "source": [
    "feature_waves = [5, 6, 7, 8, 9, 10, 11, 12]\n",
    "prediction_horizon = 2  # 4 years\n",
    "\n",
    "years = prediction_horizon * 2\n",
    "save_dir = \"data/processed\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for disease in disease_map.keys():\n",
    "\n",
    "    print(f\"\\nCreating dataset for {disease}...\")\n",
    "\n",
    "    X, y = create_dataset(\n",
    "        df=df,\n",
    "        feature_waves=feature_waves,\n",
    "        prediction_horizon=prediction_horizon,\n",
    "        target=disease\n",
    "    )\n",
    "\n",
    "    # Drop person_id if present\n",
    "    X = X.drop(columns=['person_id'], errors='ignore')\n",
    "\n",
    "    # Ensure y is a Series\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.iloc[:, 0]\n",
    "\n",
    "    target_name = f\"incident_{disease}_{years}yr\"\n",
    "\n",
    "    dataset = pd.concat(\n",
    "        [\n",
    "            X.reset_index(drop=True),\n",
    "            y.reset_index(drop=True).rename(target_name)\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # File paths\n",
    "    csv_path = os.path.join(save_dir, f\"randhrs_{disease}_{years}yr.csv\")\n",
    "    parquet_path = os.path.join(save_dir, f\"randhrs_{disease}_{years}yr.parquet\")\n",
    "\n",
    "    # Save CSV\n",
    "    dataset.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Save Parquet\n",
    "    dataset.to_parquet(parquet_path, index=False)\n",
    "\n",
    "    print(\n",
    "        f\"{disease}: shape = {dataset.shape}, \"\n",
    "        f\"positives = {dataset[target_name].sum()}, \"\n",
    "        f\"saved to {csv_path} & {parquet_path}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

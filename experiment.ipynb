{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8adcf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    " HEA HACKATHON: \"AI in Search of Hidden Health Signals\"\n",
    " Gold-Level Disease Onset Prediction — RAND HRS Longitudinal Data\n",
    "=============================================================================\n",
    " Predicts onset of chronic disease from self-reported health, lifestyle,\n",
    " and socioeconomic signals in currently-healthy individuals.\n",
    "\n",
    " Scoring: F2-Score, PR-AUC, ROC-AUC\n",
    " Compliance: No data leakage, fairness, explainability, open-source only\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, fbeta_score, precision_recall_curve, auc,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except ImportError:\n",
    "    HAS_SHAP = False\n",
    "    print(\"[WARN] shap not installed. pip install shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "468d3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#  CONFIG\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "DATA_PATH = \"randhrs1992_2022v1.sas7bdat\"\n",
    "OUTPUT_DIR = Path.cwd() / \"output\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Waves: start at 3 so we can compute lag from wave 2\n",
    "WAVES = list(range(3, 17))\n",
    "\n",
    "# ── Variable dictionaries (verified to exist in dataset) ─────────────────\n",
    "# Maps friendly_name → RAND column suffix  (columns = R{wave}{suffix})\n",
    "\n",
    "# FEATURES — safe to use as predictors\n",
    "FEATURE_VARS = {\n",
    "    # Self-rated health\n",
    "    \"Health\":         \"SHLT\",       # 1=Excellent .. 5=Poor\n",
    "    \"HealthChange\":   \"SHLTC\",      # Health change perception\n",
    "\n",
    "    # Body\n",
    "    \"BMI\":            \"BMI\",\n",
    "\n",
    "    # Mental health\n",
    "    \"Depression\":     \"CESD\",       # CES-D 8-item (0-8)\n",
    "    \"Psych\":          \"PSYCH\",      # Psychiatric conditions flag\n",
    "\n",
    "    # Functional limitations (all verified to exist wave 2-16)\n",
    "    \"Mobility\":       \"MOBILA\",     # Mobility limitations count\n",
    "    \"LgMuscle\":       \"LGMUSA\",     # Large muscle group limitations\n",
    "    \"GrossMotor\":     \"GROSSA\",     # Gross motor limitations\n",
    "    \"FineMotor\":      \"FINEA\",      # Fine motor limitations\n",
    "\n",
    "    # Cognition (waves 3-13)\n",
    "    \"WordRecall\":     \"TR20\",       # Total word recall\n",
    "    \"MentalStatus\":   \"MSTOT\",      # Mental status summary\n",
    "\n",
    "    # Health behaviors\n",
    "    \"EverSmoked\":     \"SMOKEV\",     # Ever smoked (0/1)\n",
    "    \"SmokeNow\":       \"SMOKEN\",     # Smoke currently (0/1)\n",
    "    \"Drinks\":         \"DRINK\",      # Drinks per day\n",
    "    \"VigExercise\":    \"VGACTX\",     # Vigorous exercise (0/1, waves 7-16)\n",
    "\n",
    "    # Healthcare utilization\n",
    "    \"Hospital\":       \"HOSP\",       # Hospital stay since last wave (0/1)\n",
    "    \"DoctorVisits\":   \"DOCTIM\",     # Number of doctor visits\n",
    "    \"OOPMedical\":     \"OOPMD\",      # Out-of-pocket medical expenses\n",
    "\n",
    "    # Blood pressure as RISK FACTOR (not target)\n",
    "    \"HighBP\":         \"HIBPE\",      # Ever had high blood pressure\n",
    "\n",
    "    # Other risk\n",
    "    \"ConditionCount\": \"CONDE\",      # Count of chronic conditions (composite)\n",
    "\n",
    "    # Age\n",
    "    \"Age\":            \"AGEY_E\",\n",
    "\n",
    "    # Socioeconomic\n",
    "    \"Working\":        \"WORK\",       # Currently working (0/1)\n",
    "    \"MaritalStatus\":  \"MSTAT\",      # Marital status code\n",
    "}\n",
    "\n",
    "# TARGETS — used only to build Y labels, NEVER as features\n",
    "TARGET_VARS = {\n",
    "    \"T_Diabetes\":  \"DIABE\",     # Ever had diabetes\n",
    "    \"T_Heart\":     \"HEARTE\",    # Ever had heart problems\n",
    "    \"T_Stroke\":    \"STROKE\",    # Ever had stroke\n",
    "    \"T_Lung\":      \"LUNGE\",     # Ever had lung disease\n",
    "    \"T_Cancer\":    \"CANCRE\",    # Ever had cancer\n",
    "    \"T_Arthritis\": \"ARTHRE\",    # Ever had arthritis\n",
    "}\n",
    "\n",
    "# Static demographics\n",
    "STATIC_VARS = [\"HHIDPN\", \"RAGENDER\", \"RARACEM\", \"RAHISPAN\", \"RAEDYRS\"]\n",
    "\n",
    "def banner(msg):\n",
    "    print(f\"\\n{'='*70}\\n  {msg}\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74fb67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#  PHASE 1: LOAD & RESHAPE\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_and_reshape(data_path):\n",
    "    banner(\"PHASE 1: DATA LOADING & VARIABLE EXTRACTION\")\n",
    "    print(f\"Loading: {data_path}\")\n",
    "    df_raw = pd.read_sas(str(data_path))\n",
    "    print(f\"Raw data: {df_raw.shape[0]:,} x {df_raw.shape[1]:,}\")\n",
    "\n",
    "    # Static demographics\n",
    "    avail_static = [c for c in STATIC_VARS if c in df_raw.columns]\n",
    "    df_static = df_raw[avail_static].copy()\n",
    "    print(f\"Static vars: {avail_static}\")\n",
    "\n",
    "    all_vars = {**FEATURE_VARS, **TARGET_VARS}\n",
    "    long_frames = []\n",
    "\n",
    "    for wave in WAVES:\n",
    "        wave_data = {\"HHIDPN\": df_raw[\"HHIDPN\"]}\n",
    "        found = 0\n",
    "        for name, suffix in all_vars.items():\n",
    "            col = f\"R{wave}{suffix}\"\n",
    "            if col in df_raw.columns:\n",
    "                wave_data[name] = df_raw[col].values\n",
    "                found += 1\n",
    "            else:\n",
    "                wave_data[name] = np.nan\n",
    "        if found < 5:\n",
    "            print(f\"  Wave {wave}: {found} vars — SKIPPED\")\n",
    "            continue\n",
    "        df_w = pd.DataFrame(wave_data)\n",
    "        df_w[\"Wave\"] = wave\n",
    "        long_frames.append(df_w)\n",
    "        print(f\"  Wave {wave}: {found}/{len(all_vars)} vars\")\n",
    "\n",
    "    df_long = pd.concat(long_frames, ignore_index=True)\n",
    "    df_long = pd.merge(df_long, df_static, on=\"HHIDPN\", how=\"left\")\n",
    "    df_long.sort_values([\"HHIDPN\", \"Wave\"], inplace=True)\n",
    "    df_long.reset_index(drop=True, inplace=True)\n",
    "    print(f\"\\nLong format: {df_long.shape[0]:,} rows x {df_long.shape[1]} cols\")\n",
    "    return df_long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37e0987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#  PHASE 2: FEATURE ENGINEERING\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def engineer_features(df):\n",
    "    banner(\"PHASE 2: FEATURE ENGINEERING\")\n",
    "    df = df.sort_values([\"HHIDPN\", \"Wave\"]).copy()\n",
    "\n",
    "    feat_cols = list(FEATURE_VARS.keys())\n",
    "    tgt_cols = list(TARGET_VARS.keys())\n",
    "\n",
    "    # Forward-fill within person\n",
    "    print(\"Forward-filling missing values...\")\n",
    "    for c in feat_cols + tgt_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df.groupby(\"HHIDPN\")[c].ffill()\n",
    "\n",
    "    # ── Build TARGET ──────────────────────────────────────────────────────\n",
    "    print(\"Building target: disease onset...\")\n",
    "    for tc in tgt_cols:\n",
    "        if tc in df.columns:\n",
    "            df[tc] = df[tc].fillna(0).clip(0, 1)\n",
    "\n",
    "    df[\"CurrentDiseaseCount\"] = df[tgt_cols].sum(axis=1)\n",
    "    df[\"IsSick\"] = (df[\"CurrentDiseaseCount\"] > 0).astype(int)\n",
    "    df[\"NextSick\"] = df.groupby(\"HHIDPN\")[\"IsSick\"].shift(-1)\n",
    "\n",
    "    # Onset: healthy now → sick next wave\n",
    "    df[\"Y\"] = 0\n",
    "    df.loc[(df[\"IsSick\"] == 0) & (df[\"NextSick\"] == 1), \"Y\"] = 1\n",
    "\n",
    "    # Filter: only at-risk (currently healthy + known future)\n",
    "    df = df[(df[\"IsSick\"] == 0) & (df[\"NextSick\"].notna())].copy()\n",
    "    print(f\"At-risk cohort: {len(df):,} | Event rate: {df['Y'].mean():.2%}\")\n",
    "\n",
    "    # ── Remove all-NaN feature columns before engineering ─────────────────\n",
    "    for c in feat_cols[:]:\n",
    "        if c in df.columns and df[c].isna().all():\n",
    "            print(f\"  Dropping all-NaN column: {c}\")\n",
    "            df.drop(columns=[c], inplace=True)\n",
    "            feat_cols.remove(c)\n",
    "\n",
    "    # ── Continuous variables for temporal features ────────────────────────\n",
    "    continuous = [c for c in [\n",
    "        \"BMI\", \"Depression\", \"Mobility\", \"Health\", \"Age\",\n",
    "        \"LgMuscle\", \"GrossMotor\", \"FineMotor\", \"WordRecall\",\n",
    "        \"MentalStatus\", \"Drinks\", \"DoctorVisits\", \"OOPMedical\",\n",
    "        \"ConditionCount\", \"HealthChange\"\n",
    "    ] if c in df.columns]\n",
    "\n",
    "    print(f\"Temporal features on {len(continuous)} continuous vars...\")\n",
    "    for col in continuous:\n",
    "        g = df.groupby(\"HHIDPN\")[col]\n",
    "        df[f\"D_{col}\"] = g.diff()\n",
    "        df[f\"L1_{col}\"] = g.shift(1)\n",
    "        df[f\"L2_{col}\"] = g.shift(2)\n",
    "        df[f\"A_{col}\"] = df.groupby(\"HHIDPN\")[f\"D_{col}\"].diff()\n",
    "\n",
    "    # ── Rolling statistics (3-wave) ───────────────────────────────────────\n",
    "    rolling = [c for c in [\"BMI\",\"Depression\",\"Health\",\"Mobility\"] if c in df.columns]\n",
    "    print(f\"Rolling stats on {len(rolling)} vars...\")\n",
    "    for col in rolling:\n",
    "        g = df.groupby(\"HHIDPN\")[col]\n",
    "        df[f\"R3m_{col}\"] = g.transform(lambda x: x.rolling(3, min_periods=1).mean())\n",
    "        df[f\"R3s_{col}\"] = g.transform(lambda x: x.rolling(3, min_periods=1).std())\n",
    "        # Min/max over rolling window\n",
    "        df[f\"R3min_{col}\"] = g.transform(lambda x: x.rolling(3, min_periods=1).min())\n",
    "        df[f\"R3max_{col}\"] = g.transform(lambda x: x.rolling(3, min_periods=1).max())\n",
    "        # Range\n",
    "        df[f\"R3range_{col}\"] = df[f\"R3max_{col}\"] - df[f\"R3min_{col}\"]\n",
    "\n",
    "    # ── Cross-domain interactions ─────────────────────────────────────────\n",
    "    print(\"Cross-domain interactions...\")\n",
    "    def safe_mul(a, b):\n",
    "        aa = df.get(a)\n",
    "        bb = df.get(b)\n",
    "        if aa is not None and bb is not None:\n",
    "            return aa * bb\n",
    "        return None\n",
    "\n",
    "    interactions = {\n",
    "        \"Ix_AgeBMI\": (\"Age\", \"BMI\"),\n",
    "        \"Ix_AgeDep\": (\"Age\", \"Depression\"),\n",
    "        \"Ix_AgeHealth\": (\"Age\", \"Health\"),\n",
    "        \"Ix_DepHealth\": (\"Depression\", \"Health\"),\n",
    "        \"Ix_BMIMob\": (\"BMI\", \"Mobility\"),\n",
    "        \"Ix_BMIDep\": (\"BMI\", \"Depression\"),\n",
    "        \"Ix_AgeMob\": (\"Age\", \"Mobility\"),\n",
    "        \"Ix_AgeCondCnt\": (\"Age\", \"ConditionCount\"),\n",
    "        \"Ix_HealthMob\": (\"Health\", \"Mobility\"),\n",
    "        \"Ix_DepMob\": (\"Depression\", \"Mobility\"),\n",
    "    }\n",
    "    for name, (a, b) in interactions.items():\n",
    "        r = safe_mul(a, b)\n",
    "        if r is not None:\n",
    "            df[name] = r\n",
    "\n",
    "    # Age non-linearity\n",
    "    if \"Age\" in df.columns:\n",
    "        df[\"AgeSq\"] = df[\"Age\"] ** 2\n",
    "        # Age bins\n",
    "        df[\"AgeBin\"] = pd.cut(df[\"Age\"], bins=[0,55,60,65,70,75,80,85,120],\n",
    "                              labels=list(range(8))).astype(float)\n",
    "\n",
    "    # BMI categories\n",
    "    if \"BMI\" in df.columns:\n",
    "        df[\"BMI_Cat\"] = pd.cut(df[\"BMI\"], bins=[0,18.5,25,30,35,100],\n",
    "                               labels=[0,1,2,3,4]).astype(float)\n",
    "\n",
    "    # ── Threshold flags ───────────────────────────────────────────────────\n",
    "    print(\"Threshold flags...\")\n",
    "    if \"BMI\" in df.columns:\n",
    "        df[\"Fl_Obese\"] = (df[\"BMI\"] >= 30).astype(int)\n",
    "        df[\"Fl_Underweight\"] = (df[\"BMI\"] < 18.5).astype(int)\n",
    "        df[\"Fl_Overweight\"] = (df[\"BMI\"] >= 25).astype(int)\n",
    "    if \"Depression\" in df.columns:\n",
    "        df[\"Fl_Depressed\"] = (df[\"Depression\"] >= 4).astype(int)\n",
    "        df[\"Fl_MildDep\"] = (df[\"Depression\"] >= 2).astype(int)\n",
    "    if \"Health\" in df.columns:\n",
    "        df[\"Fl_PoorHealth\"] = (df[\"Health\"] >= 4).astype(int)\n",
    "        df[\"Fl_FairHealth\"] = (df[\"Health\"] >= 3).astype(int)\n",
    "    if \"Mobility\" in df.columns:\n",
    "        l1_mob = df.get(\"L1_Mobility\")\n",
    "        if l1_mob is not None:\n",
    "            df[\"Fl_MobCrisis\"] = ((df[\"Mobility\"] > 0) & (l1_mob.fillna(0) == 0)).astype(int)\n",
    "    if \"D_Health\" in df.columns:\n",
    "        df[\"Fl_HealthDrop\"] = (df[\"D_Health\"] >= 2).astype(int)\n",
    "        df[\"Fl_HealthDrop1\"] = (df[\"D_Health\"] >= 1).astype(int)\n",
    "    if \"D_Depression\" in df.columns:\n",
    "        df[\"Fl_DepSpike\"] = (df[\"D_Depression\"] >= 3).astype(int)\n",
    "    if \"D_BMI\" in df.columns:\n",
    "        df[\"Fl_BMISurge\"] = (df[\"D_BMI\"].abs() >= 3).astype(int)\n",
    "\n",
    "    # ── Cumulative exposure ───────────────────────────────────────────────\n",
    "    print(\"Cumulative exposure...\")\n",
    "    cum_cols = {\n",
    "        \"CumObese\": \"Fl_Obese\",\n",
    "        \"CumDepressed\": \"Fl_Depressed\",\n",
    "        \"CumPoorH\": \"Fl_PoorHealth\",\n",
    "        \"CumHosp\": \"Hospital\",\n",
    "    }\n",
    "    for new_name, src in cum_cols.items():\n",
    "        if src in df.columns:\n",
    "            df[new_name] = df.groupby(\"HHIDPN\")[src].cumsum()\n",
    "\n",
    "    # ── Demographics encoding ─────────────────────────────────────────────\n",
    "    print(\"Demographics...\")\n",
    "    if \"RAGENDER\" in df.columns:\n",
    "        df[\"IsFemale\"] = (df[\"RAGENDER\"] == 2).astype(int)\n",
    "    if \"RARACEM\" in df.columns:\n",
    "        df[\"RaceWhite\"] = (df[\"RARACEM\"] == 1).astype(int)\n",
    "        df[\"RaceBlack\"] = (df[\"RARACEM\"] == 2).astype(int)\n",
    "        df[\"RaceOther\"] = (df[\"RARACEM\"] == 3).astype(int)\n",
    "    if \"RAHISPAN\" in df.columns:\n",
    "        df[\"IsHispanic\"] = (df[\"RAHISPAN\"] == 1).astype(int)\n",
    "    if \"RAEDYRS\" in df.columns:\n",
    "        df[\"Education\"] = df[\"RAEDYRS\"]\n",
    "        df[\"Fl_LowEduc\"] = (df[\"RAEDYRS\"] < 12).astype(int)\n",
    "\n",
    "    # ── Health trajectory ─────────────────────────────────────────────────\n",
    "    if \"D_Health\" in df.columns:\n",
    "        df[\"HealthTraj\"] = 0\n",
    "        df.loc[df[\"D_Health\"] > 0.5, \"HealthTraj\"] = 1\n",
    "        df.loc[df[\"D_Health\"] < -0.5, \"HealthTraj\"] = -1\n",
    "\n",
    "    # ── Marital status encoding ───────────────────────────────────────────\n",
    "    if \"MaritalStatus\" in df.columns:\n",
    "        df[\"IsMarried\"] = (df[\"MaritalStatus\"].isin([1, 2, 3])).astype(int)\n",
    "\n",
    "    # ── Delta interactions (rate-of-change interactions) ───────────────────\n",
    "    print(\"Delta interactions...\")\n",
    "    delta_ixs = {\n",
    "        \"Dix_BMI_Health\": (\"D_BMI\", \"D_Health\"),\n",
    "        \"Dix_Dep_Health\": (\"D_Depression\", \"D_Health\"),\n",
    "        \"Dix_BMI_Dep\": (\"D_BMI\", \"D_Depression\"),\n",
    "        \"Dix_Mob_Health\": (\"D_Mobility\", \"D_Health\"),\n",
    "    }\n",
    "    for name, (a, b) in delta_ixs.items():\n",
    "        if a in df.columns and b in df.columns:\n",
    "            df[name] = df[a] * df[b]\n",
    "\n",
    "    # ── Ratio features ────────────────────────────────────────────────────\n",
    "    print(\"Ratio features...\")\n",
    "    if \"BMI\" in df.columns and \"L1_BMI\" in df.columns:\n",
    "        df[\"BMI_Ratio\"] = df[\"BMI\"] / df[\"L1_BMI\"].replace(0, np.nan)\n",
    "    if \"Depression\" in df.columns and \"L1_Depression\" in df.columns:\n",
    "        df[\"Dep_Ratio\"] = df[\"Depression\"] / df[\"L1_Depression\"].replace(0, np.nan)\n",
    "\n",
    "    # ── Wave number as feature (captures cohort/time effects) ─────────────\n",
    "    df[\"WaveNum\"] = df[\"Wave\"]\n",
    "\n",
    "    n_feats = len([c for c in df.columns if c not in\n",
    "                   {\"HHIDPN\",\"Wave\",\"Y\",\"NextSick\",\"IsSick\",\n",
    "                    \"CurrentDiseaseCount\",\"RAGENDER\",\"RARACEM\",\n",
    "                    \"RAHISPAN\",\"RAEDYRS\"} | set(tgt_cols)])\n",
    "    print(f\"\\n>> Feature engineering complete: {n_feats} features\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b05bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#  PHASE 3: SPLIT\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def prepare_splits(df):\n",
    "    banner(\"PHASE 3: TRAIN / VALIDATION SPLIT\")\n",
    "    tgt_cols = list(TARGET_VARS.keys())\n",
    "    exclude = {\"HHIDPN\",\"Wave\",\"Y\",\"NextSick\",\"IsSick\",\n",
    "               \"CurrentDiseaseCount\",\"RAGENDER\",\"RARACEM\",\n",
    "               \"RAHISPAN\",\"RAEDYRS\",\"MaritalStatus\"} | set(tgt_cols)\n",
    "    features = sorted([c for c in df.columns if c not in exclude])\n",
    "    print(f\"Feature count: {len(features)}\")\n",
    "\n",
    "    df[\"Y\"] = df[\"Y\"].astype(int)\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "    train_idx, val_idx = next(splitter.split(df, df[\"Y\"], groups=df[\"HHIDPN\"]))\n",
    "\n",
    "    X_tr = df.iloc[train_idx][features].copy()\n",
    "    y_tr = df.iloc[train_idx][\"Y\"].copy()\n",
    "    X_va = df.iloc[val_idx][features].copy()\n",
    "    y_va = df.iloc[val_idx][\"Y\"].copy()\n",
    "    demo = df.iloc[val_idx][[\"HHIDPN\",\"RAGENDER\",\"RARACEM\",\"Age\"]].copy()\n",
    "\n",
    "    print(f\"Train: {len(X_tr):,} | Val: {len(X_va):,}\")\n",
    "    print(f\"Train event: {y_tr.mean():.2%} | Val event: {y_va.mean():.2%}\")\n",
    "    return features, X_tr, y_tr, X_va, y_va, demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac7b5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#  PHASE 4: MULTI-MODEL ENSEMBLE\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def train_models(X_tr, y_tr, X_va, y_va, features):\n",
    "    banner(\"PHASE 4: MULTI-MODEL ENSEMBLE\")\n",
    "\n",
    "    neg = (y_tr==0).sum()\n",
    "    pos = (y_tr==1).sum()\n",
    "    ratio = neg / max(pos, 1)\n",
    "    print(f\"Imbalance: {neg:,} neg / {pos:,} pos = {ratio:.1f}:1\")\n",
    "\n",
    "    models = {}\n",
    "    preds = {}\n",
    "\n",
    "    # ── LightGBM ──────────────────────────────────────────────────────────\n",
    "    print(\"\\n--- LightGBM ---\")\n",
    "    lgb = LGBMClassifier(\n",
    "        n_estimators=5000, learning_rate=0.005, max_depth=7, num_leaves=63,\n",
    "        min_child_samples=30, colsample_bytree=0.6, subsample=0.7,\n",
    "        subsample_freq=5, reg_alpha=0.1, reg_lambda=3.0,\n",
    "        scale_pos_weight=ratio,\n",
    "        random_state=SEED, n_jobs=-1, verbose=-1,\n",
    "    )\n",
    "    lgb.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n",
    "            callbacks=[\n",
    "                __import__(\"lightgbm\").early_stopping(300, verbose=True),\n",
    "                __import__(\"lightgbm\").log_evaluation(500),\n",
    "            ])\n",
    "    p_lgb = lgb.predict_proba(X_va)[:, 1]\n",
    "    models[\"LightGBM\"] = lgb\n",
    "    preds[\"LightGBM\"] = p_lgb\n",
    "    print(f\"LightGBM ROC-AUC: {roc_auc_score(y_va, p_lgb):.4f}\")\n",
    "\n",
    "    # ── LightGBM v2 (different hyperparameters for diversity) ─────────────\n",
    "    print(\"\\n--- LightGBM_v2 ---\")\n",
    "    lgb2 = LGBMClassifier(\n",
    "        n_estimators=5000, learning_rate=0.01, max_depth=5, num_leaves=31,\n",
    "        min_child_samples=50, colsample_bytree=0.5, subsample=0.8,\n",
    "        subsample_freq=3, reg_alpha=0.5, reg_lambda=5.0,\n",
    "        scale_pos_weight=ratio * 0.8,  # slightly less aggressive\n",
    "        random_state=SEED + 1, n_jobs=-1, verbose=-1,\n",
    "    )\n",
    "    lgb2.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n",
    "             callbacks=[\n",
    "                 __import__(\"lightgbm\").early_stopping(300, verbose=True),\n",
    "                 __import__(\"lightgbm\").log_evaluation(500),\n",
    "             ])\n",
    "    p_lgb2 = lgb2.predict_proba(X_va)[:, 1]\n",
    "    models[\"LightGBM_v2\"] = lgb2\n",
    "    preds[\"LightGBM_v2\"] = p_lgb2\n",
    "    print(f\"LightGBM_v2 ROC-AUC: {roc_auc_score(y_va, p_lgb2):.4f}\")\n",
    "\n",
    "    # ── CatBoost ──────────────────────────────────────────────────────────\n",
    "    print(\"\\n--- CatBoost ---\")\n",
    "    cb = CatBoostClassifier(\n",
    "        iterations=5000, learning_rate=0.01, depth=6, l2_leaf_reg=5,\n",
    "        loss_function=\"Logloss\", eval_metric=\"AUC\",\n",
    "        scale_pos_weight=ratio,\n",
    "        early_stopping_rounds=300, verbose=500,\n",
    "        allow_writing_files=False, random_seed=SEED,\n",
    "    )\n",
    "    cb.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)\n",
    "    p_cb = cb.predict_proba(X_va)[:, 1]\n",
    "    models[\"CatBoost\"] = cb\n",
    "    preds[\"CatBoost\"] = p_cb\n",
    "    print(f\"CatBoost ROC-AUC: {roc_auc_score(y_va, p_cb):.4f}\")\n",
    "\n",
    "    # ── XGBoost ───────────────────────────────────────────────────────────\n",
    "    print(\"\\n--- XGBoost ---\")\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=5000, learning_rate=0.01, max_depth=6,\n",
    "        min_child_weight=10, colsample_bytree=0.6, subsample=0.7,\n",
    "        reg_alpha=0.1, reg_lambda=3.0, scale_pos_weight=ratio,\n",
    "        eval_metric=\"auc\", early_stopping_rounds=300,\n",
    "        verbosity=0, random_state=SEED, n_jobs=-1, tree_method=\"hist\",\n",
    "    )\n",
    "    xgb.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=500)\n",
    "    p_xgb = xgb.predict_proba(X_va)[:, 1]\n",
    "    models[\"XGBoost\"] = xgb\n",
    "    preds[\"XGBoost\"] = p_xgb\n",
    "    print(f\"XGBoost ROC-AUC: {roc_auc_score(y_va, p_xgb):.4f}\")\n",
    "\n",
    "    # ── Stacking Ensemble ─────────────────────────────────────────────────\n",
    "    print(\"\\n--- Ensemble ---\")\n",
    "    base_names = [\"LightGBM\", \"LightGBM_v2\", \"CatBoost\", \"XGBoost\"]\n",
    "    stk_tr = np.column_stack([m.predict_proba(X_tr)[:,1] for m in [models[n] for n in base_names]])\n",
    "    stk_va = np.column_stack([preds[n] for n in base_names])\n",
    "\n",
    "    meta = LogisticRegression(C=1.0, max_iter=1000, random_state=SEED)\n",
    "    meta.fit(stk_tr, y_tr)\n",
    "    p_ens = meta.predict_proba(stk_va)[:, 1]\n",
    "    models[\"Ensemble_Meta\"] = meta\n",
    "    preds[\"Ensemble\"] = p_ens\n",
    "    print(f\"Ensemble ROC-AUC: {roc_auc_score(y_va, p_ens):.4f}\")\n",
    "\n",
    "    # Simple average\n",
    "    p_avg = np.mean(stk_va, axis=1)\n",
    "    preds[\"Average\"] = p_avg\n",
    "    print(f\"Average  ROC-AUC: {roc_auc_score(y_va, p_avg):.4f}\")\n",
    "\n",
    "    # Rank average (robust ensemble)\n",
    "    from scipy.stats import rankdata\n",
    "    ranks = np.column_stack([rankdata(preds[n]) for n in base_names])\n",
    "    p_rank = ranks.mean(axis=1) / len(y_va)\n",
    "    preds[\"RankAvg\"] = p_rank\n",
    "    print(f\"RankAvg  ROC-AUC: {roc_auc_score(y_va, p_rank):.4f}\")\n",
    "\n",
    "    return models, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec724ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#  PHASE 5: THRESHOLD OPTIMIZATION\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def evaluate(y_va, predictions):\n",
    "    banner(\"PHASE 5: EVALUATION & THRESHOLD OPTIMIZATION\")\n",
    "\n",
    "    results = {}\n",
    "    best_name = None\n",
    "    best_f2 = -1\n",
    "\n",
    "    for name, probs in predictions.items():\n",
    "        roc = roc_auc_score(y_va, probs)\n",
    "        prec, rec, _ = precision_recall_curve(y_va, probs)\n",
    "        pr = auc(rec, prec)\n",
    "\n",
    "        thresholds = np.arange(0.03, 0.90, 0.005)\n",
    "        f2s = []\n",
    "        for t in thresholds:\n",
    "            p = (probs > t).astype(int)\n",
    "            f2s.append(fbeta_score(y_va, p, beta=2) if p.sum() > 0 else 0)\n",
    "        b_f2 = max(f2s)\n",
    "        b_thr = float(thresholds[np.argmax(f2s)])\n",
    "\n",
    "        print(f\"  {name:<15} ROC={roc:.4f}  PR={pr:.4f}  F2={b_f2:.4f} (t={b_thr:.3f})\")\n",
    "\n",
    "        best_p = (probs > b_thr).astype(int)\n",
    "        print(classification_report(y_va, best_p,\n",
    "              target_names=[\"Healthy\",\"Onset\"], digits=4))\n",
    "\n",
    "        results[name] = {\"roc_auc\":roc, \"pr_auc\":pr, \"best_f2\":b_f2,\n",
    "                         \"best_threshold\":b_thr, \"probs\":probs}\n",
    "        if b_f2 > best_f2:\n",
    "            best_f2 = b_f2\n",
    "            best_name = name\n",
    "\n",
    "    print(f\"\\n>> BEST: {best_name} — F2={results[best_name]['best_f2']:.4f}, \"\n",
    "          f\"ROC={results[best_name]['roc_auc']:.4f}, \"\n",
    "          f\"PR={results[best_name]['pr_auc']:.4f}\")\n",
    "    return results, best_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea08d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#  PHASE 6: FAIRNESS\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def fairness_audit(y_va, probs, thr, demo):\n",
    "    banner(\"PHASE 6: FAIRNESS AUDIT\")\n",
    "    p = (probs > thr).astype(int)\n",
    "    d = demo.copy()\n",
    "    d[\"yt\"] = y_va.values\n",
    "    d[\"yp\"] = p\n",
    "    d[\"prob\"] = probs\n",
    "\n",
    "    def check(col, labels=None):\n",
    "        print(f\"\\n  By {col}:\")\n",
    "        print(f\"  {'Group':<15} {'N':>7} {'ROC':>7} {'F2':>6} {'FPR':>6} {'FNR':>6}\")\n",
    "        for v in sorted(d[col].dropna().unique()):\n",
    "            m = d[col] == v\n",
    "            sy = d.loc[m,\"yt\"]\n",
    "            sp = d.loc[m,\"yp\"]\n",
    "            sprob = d.loc[m,\"prob\"]\n",
    "            if sy.nunique()<2 or len(sy)<50: continue\n",
    "            r = roc_auc_score(sy, sprob)\n",
    "            f = fbeta_score(sy, sp, beta=2)\n",
    "            tn,fp,fn,tp = confusion_matrix(sy,sp).ravel()\n",
    "            fpr = fp/max(fp+tn,1)\n",
    "            fnr = fn/max(fn+tp,1)\n",
    "            lbl = labels.get(v,str(v)) if labels else str(v)\n",
    "            print(f\"  {lbl:<15} {len(sy):>7,} {r:>7.4f} {f:>6.4f} {fpr:>6.4f} {fnr:>6.4f}\")\n",
    "\n",
    "    if \"RAGENDER\" in d.columns:\n",
    "        check(\"RAGENDER\", {1:\"Male\", 2:\"Female\"})\n",
    "    if \"RARACEM\" in d.columns:\n",
    "        check(\"RARACEM\", {1:\"White\", 2:\"Black\", 3:\"Other\"})\n",
    "    if \"Age\" in d.columns:\n",
    "        d[\"AB\"] = pd.cut(d[\"Age\"], bins=[0,55,65,75,85,120],\n",
    "                         labels=[\"50-55\",\"56-65\",\"66-75\",\"76-85\",\"85+\"])\n",
    "        check(\"AB\")\n",
    "    print(\"\\n  >> Fairness audit complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27752724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#  PHASE 7: EXPLAINABILITY\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def explain(model, X_va, features, name=\"LightGBM\"):\n",
    "    banner(\"PHASE 7: EXPLAINABILITY (SHAP)\")\n",
    "    if not HAS_SHAP:\n",
    "        print(\"[SKIP] shap not installed\")\n",
    "        return\n",
    "\n",
    "    print(f\"SHAP for {name}...\")\n",
    "    sample = X_va.sample(n=min(2000, len(X_va)), random_state=SEED)\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        sv = explainer.shap_values(sample)\n",
    "        if isinstance(sv, list): sv = sv[1]\n",
    "\n",
    "        plt.figure(figsize=(12,10))\n",
    "        shap.summary_plot(sv, sample, max_display=30, show=False, plot_size=(12,10))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR/\"shap_summary.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"  >> {OUTPUT_DIR/'shap_summary.png'}\")\n",
    "\n",
    "        plt.figure(figsize=(12,10))\n",
    "        shap.summary_plot(sv, sample, plot_type=\"bar\", max_display=30,\n",
    "                          show=False, plot_size=(12,10))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR/\"shap_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"  >> {OUTPUT_DIR/'shap_importance.png'}\")\n",
    "\n",
    "        hi = np.argmax(sv.sum(axis=1))\n",
    "        plt.figure(figsize=(12,8))\n",
    "        ev = explainer.expected_value\n",
    "        if not np.isscalar(ev): ev = ev[1]\n",
    "        shap.waterfall_plot(shap.Explanation(\n",
    "            values=sv[hi], base_values=ev,\n",
    "            data=sample.iloc[hi], feature_names=list(sample.columns)\n",
    "        ), max_display=20, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR/\"shap_waterfall.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"  >> {OUTPUT_DIR/'shap_waterfall.png'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] SHAP failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f121ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_importance(models, features):\n",
    "    print(\"\\n--- Feature Importance ---\")\n",
    "    dfs = []\n",
    "    for n, m in models.items():\n",
    "        if n == \"Ensemble_Meta\": continue\n",
    "        if hasattr(m, \"feature_importances_\"):\n",
    "            dfs.append(pd.DataFrame({\"Feature\":features, f\"{n}\":m.feature_importances_}))\n",
    "    if not dfs: return None\n",
    "    mg = dfs[0]\n",
    "    for d in dfs[1:]: mg = pd.merge(mg, d, on=\"Feature\", how=\"outer\")\n",
    "    icols = [c for c in mg.columns if c != \"Feature\"]\n",
    "    mg[\"Avg\"] = mg[icols].mean(axis=1)\n",
    "    mg.sort_values(\"Avg\", ascending=False, inplace=True)\n",
    "    print(mg.head(30).to_string(index=False))\n",
    "    mg.to_csv(OUTPUT_DIR/\"feature_importance.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "    top = mg.head(30)\n",
    "    plt.barh(top[\"Feature\"].values[::-1], top[\"Avg\"].values[::-1], color=\"#2196F3\")\n",
    "    plt.title(\"Top 30 Predictors of Chronic Disease Onset\", fontsize=14)\n",
    "    plt.xlabel(\"Average Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR/\"feature_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return mg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dda75a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  HEA HACKATHON — DISEASE PREDICTION PIPELINE v2\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "  PHASE 1: DATA LOADING & VARIABLE EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "Loading: randhrs1992_2022v1.sas7bdat\n",
      "Warning: column count mismatch (959 + 1802 != 19880)\n",
      "\n",
      "Raw data: 45,234 x 19,880\n",
      "Static vars: ['HHIDPN', 'RAGENDER', 'RARACEM', 'RAHISPAN', 'RAEDYRS']\n",
      "  Wave 3: 28/29 vars\n",
      "  Wave 4: 28/29 vars\n",
      "  Wave 5: 28/29 vars\n",
      "  Wave 6: 28/29 vars\n",
      "  Wave 7: 29/29 vars\n",
      "  Wave 8: 29/29 vars\n",
      "  Wave 9: 29/29 vars\n",
      "  Wave 10: 29/29 vars\n",
      "  Wave 11: 29/29 vars\n",
      "  Wave 12: 29/29 vars\n",
      "  Wave 13: 29/29 vars\n",
      "  Wave 14: 27/29 vars\n",
      "  Wave 15: 27/29 vars\n",
      "  Wave 16: 27/29 vars\n",
      "\n",
      "Long format: 633,276 rows x 35 cols\n",
      "\n",
      "======================================================================\n",
      "  PHASE 2: FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "Forward-filling missing values...\n",
      "Building target: disease onset...\n",
      "At-risk cohort: 284,295 | Event rate: 8.22%\n",
      "Temporal features on 15 continuous vars...\n",
      "Rolling stats on 4 vars...\n",
      "Cross-domain interactions...\n",
      "Threshold flags...\n",
      "Cumulative exposure...\n",
      "Demographics...\n",
      "Delta interactions...\n",
      "Ratio features...\n",
      "\n",
      ">> Feature engineering complete: 148 features\n",
      "\n",
      "======================================================================\n",
      "  PHASE 3: TRAIN / VALIDATION SPLIT\n",
      "======================================================================\n",
      "\n",
      "Feature count: 147\n",
      "Train: 227,713 | Val: 56,582\n",
      "Train event: 8.19% | Val event: 8.35%\n",
      "\n",
      "======================================================================\n",
      "  PHASE 4: MULTI-MODEL ENSEMBLE\n",
      "======================================================================\n",
      "\n",
      "Imbalance: 209,066 neg / 18,647 pos = 11.2:1\n",
      "\n",
      "--- LightGBM ---\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.280676\n",
      "LightGBM ROC-AUC: 0.7916\n",
      "\n",
      "--- LightGBM_v2 ---\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.28298\n",
      "LightGBM_v2 ROC-AUC: 0.7688\n",
      "\n",
      "--- CatBoost ---\n",
      "0:\ttest: 0.6910783\tbest: 0.6910783 (0)\ttotal: 33.7ms\tremaining: 2m 48s\n",
      "500:\ttest: 0.8039180\tbest: 0.8039180 (500)\ttotal: 12.5s\tremaining: 1m 52s\n",
      "1000:\ttest: 0.8093540\tbest: 0.8093637 (998)\ttotal: 24.3s\tremaining: 1m 36s\n",
      "1500:\ttest: 0.8115179\tbest: 0.8115179 (1500)\ttotal: 36.2s\tremaining: 1m 24s\n",
      "2000:\ttest: 0.8117998\tbest: 0.8119097 (1759)\ttotal: 47.8s\tremaining: 1m 11s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.8119096665\n",
      "bestIteration = 1759\n",
      "\n",
      "Shrink model to first 1760 iterations.\n",
      "CatBoost ROC-AUC: 0.8119\n",
      "\n",
      "--- XGBoost ---\n",
      "[0]\tvalidation_0-auc:0.62908\n",
      "[500]\tvalidation_0-auc:0.81141\n",
      "[866]\tvalidation_0-auc:0.81098\n",
      "XGBoost ROC-AUC: 0.8117\n",
      "\n",
      "--- Ensemble ---\n",
      "Ensemble ROC-AUC: 0.8078\n",
      "Average  ROC-AUC: 0.8122\n",
      "RankAvg  ROC-AUC: 0.8050\n",
      "\n",
      "======================================================================\n",
      "  PHASE 5: EVALUATION & THRESHOLD OPTIMIZATION\n",
      "======================================================================\n",
      "\n",
      "  LightGBM        ROC=0.7916  PR=0.2063  F2=0.4784 (t=0.120)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy     0.9790    0.6311    0.7675     51857\n",
      "       Onset     0.1738    0.8516    0.2887      4725\n",
      "\n",
      "    accuracy                         0.6495     56582\n",
      "   macro avg     0.5764    0.7414    0.5281     56582\n",
      "weighted avg     0.9118    0.6495    0.7275     56582\n",
      "\n",
      "  LightGBM_v2     ROC=0.7688  PR=0.1921  F2=0.4531 (t=0.115)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy     0.9759    0.5985    0.7419     51857\n",
      "       Onset     0.1597    0.8377    0.2683      4725\n",
      "\n",
      "    accuracy                         0.6184     56582\n",
      "   macro avg     0.5678    0.7181    0.5051     56582\n",
      "weighted avg     0.9077    0.6184    0.7024     56582\n",
      "\n",
      "  CatBoost        ROC=0.8119  PR=0.2719  F2=0.4852 (t=0.490)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy     0.9779    0.6541    0.7839     51857\n",
      "       Onset     0.1808    0.8379    0.2974      4725\n",
      "\n",
      "    accuracy                         0.6694     56582\n",
      "   macro avg     0.5794    0.7460    0.5406     56582\n",
      "weighted avg     0.9114    0.6694    0.7433     56582\n",
      "\n",
      "  XGBoost         ROC=0.8117  PR=0.2722  F2=0.4846 (t=0.500)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy     0.9764    0.6651    0.7913     51857\n",
      "       Onset     0.1831    0.8237    0.2996      4725\n",
      "\n",
      "    accuracy                         0.6784     56582\n",
      "   macro avg     0.5798    0.7444    0.5454     56582\n",
      "weighted avg     0.9102    0.6784    0.7502     56582\n",
      "\n",
      "  Ensemble        ROC=0.8078  PR=0.2652  F2=0.4818 (t=0.075)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy     0.9742    0.6782    0.7997     51857\n",
      "       Onset     0.1853    0.8032    0.3011      4725\n",
      "\n",
      "    accuracy                         0.6886     56582\n",
      "   macro avg     0.5798    0.7407    0.5504     56582\n",
      "weighted avg     0.9084    0.6886    0.7581     56582\n",
      "\n",
      "  Average         ROC=0.8122  PR=0.2717  F2=0.4855 (t=0.310)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy     0.9769    0.6628    0.7898     51857\n",
      "       Onset     0.1829    0.8281    0.2996      4725\n",
      "\n",
      "    accuracy                         0.6766     56582\n",
      "   macro avg     0.5799    0.7455    0.5447     56582\n",
      "weighted avg     0.9106    0.6766    0.7488     56582\n",
      "\n",
      "  RankAvg         ROC=0.8050  PR=0.2317  F2=0.4819 (t=0.610)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy     0.9783    0.6438    0.7766     51857\n",
      "       Onset     0.1775    0.8436    0.2933      4725\n",
      "\n",
      "    accuracy                         0.6605     56582\n",
      "   macro avg     0.5779    0.7437    0.5349     56582\n",
      "weighted avg     0.9115    0.6605    0.7362     56582\n",
      "\n",
      "\n",
      ">> BEST: Average — F2=0.4855, ROC=0.8122, PR=0.2717\n",
      "\n",
      "======================================================================\n",
      "  PHASE 6: FAIRNESS AUDIT\n",
      "======================================================================\n",
      "\n",
      "\n",
      "  By RAGENDER:\n",
      "  Group                 N     ROC     F2    FPR    FNR\n",
      "  Male             25,845  0.8038 0.4701 0.3351 0.1796\n",
      "  Female           30,737  0.8183 0.4977 0.3389 0.1659\n",
      "\n",
      "  By RARACEM:\n",
      "  Group                 N     ROC     F2    FPR    FNR\n",
      "  White            35,106  0.7786 0.4678 0.4043 0.1555\n",
      "  Black            13,156  0.8493 0.5272 0.2484 0.1968\n",
      "  Other             6,967  0.8493 0.4994 0.2252 0.2372\n",
      "\n",
      "  By AB:\n",
      "  Group                 N     ROC     F2    FPR    FNR\n",
      "  50-55             5,486  0.7316 0.4296 0.3940 0.2579\n",
      "  56-65             6,943  0.6781 0.4477 0.6117 0.1628\n",
      "  66-75             3,445  0.7046 0.5083 0.6752 0.0541\n",
      "  76-85             1,800  0.7214 0.5426 0.6094 0.0332\n",
      "  85+                 722  0.8043 0.5224 0.3625 0.0667\n",
      "\n",
      "  >> Fairness audit complete\n",
      "\n",
      "======================================================================\n",
      "  PHASE 7: EXPLAINABILITY (SHAP)\n",
      "======================================================================\n",
      "\n",
      "SHAP for LightGBM...\n",
      "  >> c:\\Users\\paypa\\Downloads\\randhrs1992_2022v1_SAS\\output\\shap_summary.png\n",
      "  >> c:\\Users\\paypa\\Downloads\\randhrs1992_2022v1_SAS\\output\\shap_importance.png\n",
      "  >> c:\\Users\\paypa\\Downloads\\randhrs1992_2022v1_SAS\\output\\shap_waterfall.png\n",
      "\n",
      "--- Feature Importance ---\n",
      "        Feature  LightGBM  LightGBM_v2  CatBoost  XGBoost       Avg\n",
      "        WaveNum        93           29 59.381333 0.064866 45.361550\n",
      "      Education       100            6  3.863689 0.009203 27.468223\n",
      "   Ix_AgeHealth        63           24  0.841830 0.021064 21.965723\n",
      "      Ix_AgeBMI        64           14  0.778130 0.008835 19.696741\n",
      "            Age        54            6  0.492107 0.023242 15.128837\n",
      "      RaceWhite        49            5  2.499579 0.020217 14.129949\n",
      "     IsHispanic        39            7  1.551731 0.012891 11.891156\n",
      "   A_OOPMedical        29           13  0.394302 0.018088 10.603097\n",
      "         L1_BMI        36            4  0.123578 0.004150 10.031932\n",
      "          AgeSq        25           14  0.500420 0.020844  9.880316\n",
      "  Ix_AgeCondCnt        26            8  0.436259 0.008161  8.611105\n",
      "   DoctorVisits        23            9  0.782178 0.007647  8.197456\n",
      "       LgMuscle        24            7  0.807260 0.012466  7.954932\n",
      "         L2_Age        23            8  0.501200 0.004167  7.876342\n",
      "         L1_Age        26            3  0.291299 0.004684  7.323996\n",
      "        R3s_BMI        24            4  0.530767 0.015331  7.136525\n",
      "          A_Age        19            6  0.452482 0.031979  6.371115\n",
      "            BMI        16            9  0.266670 0.003754  6.317606\n",
      "        R3m_BMI        21            4  0.246775 0.004411  6.312797\n",
      "  L1_OOPMedical        25            0  0.139078 0.003457  6.285634\n",
      "         L2_BMI        21            3  0.082236 0.003946  6.021546\n",
      "          D_Age        12            7  4.255876 0.060359  5.829059\n",
      "L1_DoctorVisits        21            2  0.177705 0.004508  5.795553\n",
      "  L2_OOPMedical        22            1  0.067005 0.003246  5.767563\n",
      "    VigExercise        19            1  2.181923 0.021281  5.550801\n",
      "     OOPMedical        18            1  0.297561 0.005554  4.825779\n",
      "      Ix_AgeMob        13            4  0.229143 0.005706  4.308712\n",
      "      R3max_BMI        11            4  0.378756 0.005723  3.846120\n",
      " A_DoctorVisits        13            2  0.195611 0.008493  3.801026\n",
      "      Ix_BMIMob         9            5  0.086729 0.004690  3.522855\n",
      "\n",
      "======================================================================\n",
      "  FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Model               ROC      PR      F2    Thr\n",
      "---------------------------------------------\n",
      "LightGBM         0.7916  0.2063  0.4784  0.120\n",
      "LightGBM_v2      0.7688  0.1921  0.4531  0.115\n",
      "CatBoost         0.8119  0.2719  0.4852  0.490\n",
      "XGBoost          0.8117  0.2722  0.4846  0.500\n",
      "Ensemble         0.8078  0.2652  0.4818  0.075\n",
      "Average          0.8122  0.2717  0.4855  0.310\n",
      "RankAvg          0.8050  0.2317  0.4819  0.610\n",
      "\n",
      ">> BEST: Average — ROC=0.8122 PR=0.2717 F2=0.4855\n",
      "\n",
      ">> All outputs in: c:\\Users\\paypa\\Downloads\\randhrs1992_2022v1_SAS\\output\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#  MAIN\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def main():\n",
    "    banner(\"HEA HACKATHON — DISEASE PREDICTION PIPELINE v2\")\n",
    "    import gc\n",
    "\n",
    "    df = load_and_reshape(DATA_PATH)\n",
    "    df = engineer_features(df)\n",
    "    gc.collect()\n",
    "\n",
    "    features, X_tr, y_tr, X_va, y_va, demo = prepare_splits(df)\n",
    "    del df; gc.collect()\n",
    "\n",
    "    models, predictions = train_models(X_tr, y_tr, X_va, y_va, features)\n",
    "    results, best = evaluate(y_va, predictions)\n",
    "    fairness_audit(y_va, results[best][\"probs\"], results[best][\"best_threshold\"], demo)\n",
    "\n",
    "    if \"LightGBM\" in models:\n",
    "        explain(models[\"LightGBM\"], X_va, features, \"LightGBM\")\n",
    "    export_importance(models, features)\n",
    "\n",
    "    banner(\"FINAL SUMMARY\")\n",
    "    print(f\"{'Model':<15} {'ROC':>7} {'PR':>7} {'F2':>7} {'Thr':>6}\")\n",
    "    print(\"-\"*45)\n",
    "    for n, r in results.items():\n",
    "        print(f\"{n:<15} {r['roc_auc']:>7.4f} {r['pr_auc']:>7.4f} \"\n",
    "              f\"{r['best_f2']:>7.4f} {r['best_threshold']:>6.3f}\")\n",
    "    print(f\"\\n>> BEST: {best} — \"\n",
    "          f\"ROC={results[best]['roc_auc']:.4f} \"\n",
    "          f\"PR={results[best]['pr_auc']:.4f} \"\n",
    "          f\"F2={results[best]['best_f2']:.4f}\")\n",
    "\n",
    "    rj = {n:{k:float(v) for k,v in r.items() if k!=\"probs\"} for n,r in results.items()}\n",
    "    with open(OUTPUT_DIR/\"results.json\",\"w\") as f: json.dump(rj, f, indent=2)\n",
    "    print(f\"\\n>> All outputs in: {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
